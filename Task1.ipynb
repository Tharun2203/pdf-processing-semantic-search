{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee28c96-62fd-4128-a0fa-c2a486827734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (3.0.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (4.47.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (0.3.12)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (2.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.26.5)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (3.11.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.25)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.3)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.2.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (75.5.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 faiss-cpu sentence-transformers transformers langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "094f2d84-9cd6-42dd-be7a-419181550116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Splitting text into chunks...\n",
      "Number of chunks: 101\n",
      "Embedding and storing chunks...\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0652cb34acd04e80b24392f95e4c7ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index size: 101\n",
      "Searching relevant chunks...\n",
      "Search Results - Distances: [[0.9001344  0.99176013 1.0147076 ]], Indices: [[16  4 68]]\n",
      "Generating response...\n",
      "Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      "Context: To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "\n",
      "Question: How does the Transformer compare to RNNs and LSTMs?\n",
      "\n",
      "Answer: There are several factors, including\n",
      "the number of sentences. The transformer has only one input;\n",
      "the number of sentences. The two nodes can combine in a\n",
      "theta-tunnel state and the transformer has only a fixed\n",
      "number of sentences [30]. This is the most efficient and consistent approach for generating data based on multiple-dimensional vector representation.\n",
      "There are a number of approaches to these problems, including\n",
      "for learning more about the transformer,\n",
      "and for finding more computational and computational information that is possible when using\n",
      "all of these approaches.\n",
      "4 This technique could be applied to large-data\n",
      "programmed data processing architectures.\n",
      "How can the Transformer be trained?\n",
      "Answer: Neural recurrent learning training is the\n",
      "best of these problems in general. In this study,\n",
      "that is, it is better to\n",
      "analyze data that is a fully-trained (in part) of the Neural Convolution. Because\n",
      "we will be analyzing neural connections,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "# STEP 1: Extract Text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text\n",
    "    return text\n",
    "\n",
    "# STEP 2: Split Text into Chunks\n",
    "def split_text_into_chunks(text, chunk_size=500, chunk_overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "# STEP 3: Generate Embeddings and Store in FAISS\n",
    "def embed_and_store(chunks, model_name):\n",
    "    print(\"Generating embeddings...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    \n",
    "    # Build FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(embeddings).astype('float32'))\n",
    "    print(f\"FAISS index size: {index.ntotal}\")\n",
    "    return index, model\n",
    "\n",
    "# STEP 4: Search FAISS for Relevant Chunks\n",
    "def search_similar_chunks(query, chunks, index, model, top_k=3):\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    print(f\"Search Results - Distances: {distances}, Indices: {indices}\")\n",
    "    results = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
    "    return results\n",
    "\n",
    "# STEP 5: Use Hugging Face Model for Final Response Generation\n",
    "def generate_response(context_chunks, query, hf_model_name=\"distilgpt2\"):\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "    input_text = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    print(\"Generating response...\")\n",
    "\n",
    "    hf_pipeline = pipeline(\"text-generation\", model=hf_model_name, max_new_tokens=200)\n",
    "    response = hf_pipeline(input_text)[0]['generated_text']\n",
    "    return response\n",
    "\n",
    "# MAIN PIPELINE\n",
    "def main(pdf_path, user_query, embedding_model_name=\"all-MiniLM-L6-v2\", hf_model_name=\"distilgpt2\"):\n",
    "    # Step 1: Extract text\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Step 2: Split text\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    chunks = split_text_into_chunks(pdf_text)\n",
    "    if not chunks:\n",
    "        print(\"No text chunks found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Embed and store in FAISS\n",
    "    print(\"Embedding and storing chunks...\")\n",
    "    index, model = embed_and_store(chunks, embedding_model_name)\n",
    "    \n",
    "    # Step 4: Search for relevant chunks\n",
    "    print(\"Searching relevant chunks...\")\n",
    "    similar_chunks = search_similar_chunks(user_query, chunks, index, model)\n",
    "    if not similar_chunks:\n",
    "        print(\"No relevant chunks found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 5: Generate response\n",
    "    print(\"Generating response...\")\n",
    "    response = generate_response(similar_chunks, user_query, hf_model_name)\n",
    "    print(\"\\nFinal Response:\")\n",
    "    print(response)\n",
    "\n",
    "# RUN THE PIPELINE\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"sample.pdf\"  # Your uploaded file path\n",
    "    user_query = \"How does the Transformer compare to RNNs and LSTMs?\"  # Example query\n",
    "    main(pdf_path, user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef9ff6-d86d-440b-b81a-ac5291a24927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6b768-838b-4bdf-852a-d3d4665218b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
